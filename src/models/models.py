"""
AI ëª¨ë¸ ê´€ë ¨ í´ë˜ìŠ¤ë“¤
YOLOv5ì™€ CLIP ëª¨ë¸ì„ í™œìš©í•œ ì˜· íƒì§€ ë° ìŠ¤íƒ€ì¼ ë¶„ì„
"""

import torch
import numpy as np
from PIL import Image
from ultralytics import YOLO
from transformers import CLIPProcessor, CLIPModel
from config import YOLO_MODEL_PATH, CLIP_MODEL_NAME, FASHION_CLASSES
import os

class YOLODetector:
    """YOLOv5ë¥¼ ì‚¬ìš©í•œ ì˜· ì•„ì´í…œ íƒì§€ í´ë˜ìŠ¤"""
    
    # ì˜ì–´ í´ë˜ìŠ¤ ì´ë¦„ â†’ í•œêµ­ì–´ ë§¤í•‘ (DeepFashion2 13ê°œ í´ë˜ìŠ¤)
    FASHION_CLASS_MAPPING = {
        "long sleeve dress": "ê¸´íŒ” ë“œë ˆìŠ¤",
        "long sleeve outwear": "ê¸´íŒ” ì•„ìš°í„°",
        "long sleeve top": "ê¸´íŒ” ìƒì˜",
        "short sleeve dress": "ë°˜íŒ” ë“œë ˆìŠ¤",
        "short sleeve outwear": "ë°˜íŒ” ì•„ìš°í„°",
        "short sleeve top": "ë°˜íŒ” ìƒì˜",
        "shorts": "ë°˜ë°”ì§€",
        "skirt": "ìŠ¤ì»¤íŠ¸",
        "sling dress": "ëˆ ë“œë ˆìŠ¤",
        "sling": "ëˆ ìƒì˜",
        "trousers": "ë°”ì§€",
        "vest dress": "ì¡°ë¼ ë“œë ˆìŠ¤",
        "vest": "ì¡°ë¼"
    }
    
    def __init__(self, model_path=None):
        """YOLOv5 ëª¨ë¸ ì´ˆê¸°í™”"""
        if model_path is None:
            model_path = YOLO_MODEL_PATH
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (ë©”íƒ€ í…ì„œ ë¬¸ì œ ë°©ì§€ - CPU ìš°ì„  ì‚¬ìš©)
        # ultralytics ë‚´ë¶€ì ìœ¼ë¡œ deviceë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
        device = "cpu"  # ë©”íƒ€ í…ì„œ ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ CPU ì‚¬ìš© (ë‚˜ì¤‘ì— í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
        
        # ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© (yolov5n, yolov5s ë“±)
        if not os.path.exists(model_path):
            print(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            print("ì‚¬ì „ í•™ìŠµëœ YOLOv5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: yolov5n")
            # COCO ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì‚¬ìš© (person, bag ë“± ì¼ë°˜ ê°ì²´ íƒì§€)
            try:
                # deviceë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
                self.model = YOLO('yolov5n.pt')
                # ultralyticsëŠ” ë‚´ë¶€ì ìœ¼ë¡œ deviceë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ .to() í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ğŸ’¡ ì—ëŸ¬ê°€ ì§€ì†ë˜ë©´ ì•±ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.")
                raise
            self.is_fashion_model = False
            print("ì¼ë°˜ ê°ì²´ íƒì§€ ëª¨ë¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤. íŒ¨ì…˜ ì „ìš© ëª¨ë¸ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            try:
                # íŒ¨ì…˜ ëª¨ë¸ ë¡œë“œ
                # ultralytics YOLOëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•  ë•Œ deviceë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬
                # ë©”íƒ€ í…ì„œ ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œí•˜ì§€ ì•Šê³  
                # ultralyticsì˜ ë‚´ì¥ ë¡œë”© ë°©ì‹ì„ ì‹ ë¢°
                self.model = YOLO(model_path)
                # ëª¨ë¸ì´ ì™„ì „íˆ ë¡œë“œë˜ë©´ ë‚´ë¶€ ëª¨ë¸ ê°ì²´ì— ì ‘ê·¼
                # device ì´ë™ì€ ultralyticsê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            except NotImplementedError as meta_error:
                # ë©”íƒ€ í…ì„œ ì˜¤ë¥˜ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                if "meta tensor" in str(meta_error).lower():
                    print(f"âš ï¸ ë©”íƒ€ í…ì„œ ë¬¸ì œ ê°ì§€: {meta_error}")
                    print("ğŸ’¡ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ í•™ìŠµëœ ëª¨ë¸ì„ í™•ì¸í•˜ì„¸ìš”.")
                    print("ğŸ’¡ ì„ì‹œë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
                    self.model = YOLO('yolov5n.pt')
                    self.is_fashion_model = False
                    return
                else:
                    raise
            except Exception as e:
                print(f"âš ï¸ íŒ¨ì…˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œ ëŒ€ì²´...")
                try:
                    self.model = YOLO('yolov5n.pt')
                    self.is_fashion_model = False
                except Exception as e2:
                    print(f"âš ï¸ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                    raise
                return
            
            self.is_fashion_model = True
            print(f"YOLOv5 íŒ¨ì…˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            # í•™ìŠµëœ í´ë˜ìŠ¤ í™•ì¸
            if hasattr(self.model, 'names') and self.model.names:
                classes_list = list(self.model.names.values())
                print(f"íƒì§€ ê°€ëŠ¥í•œ í´ë˜ìŠ¤: {classes_list[:5]}...")
    
    def detect_clothes(self, image, clip_analyzer=None):
        """ì´ë¯¸ì§€ì—ì„œ ì˜· ì•„ì´í…œ íƒì§€ (CLIP ê²€ì¦ í¬í•¨)"""
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if isinstance(image, Image.Image):
            img_array = np.array(image)
            pil_image = image
        elif isinstance(image, np.ndarray):
            img_array = image
            pil_image = Image.fromarray(img_array)
        else:
            raise ValueError(f"Unsupported image type: {type(image)}")
        
        # YOLOv5 ì¶”ë¡ 
        results = self.model(img_array, verbose=False)
        
        # ê²°ê³¼ íŒŒì‹±
        detected_items = []
        if len(results) > 0 and results[0].boxes is not None:
            for box in results[0].boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                bbox = box.xyxy[0].cpu().numpy().tolist()
                
                # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                class_name = self.model.names[class_id]
                
                # íŒ¨ì…˜ ëª¨ë¸ì¸ ê²½ìš°: ëª¨ë“  íƒì§€ ê²°ê³¼ ì‚¬ìš© (ì´ë¯¸ íŒ¨ì…˜ ì•„ì´í…œë§Œ íƒì§€)
                if self.is_fashion_model:
                    # ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì • (ìƒí–¥ ì¡°ì •)
                    if confidence > 0.3:  # ì„ê³„ê°’ ìƒí–¥ (0.25 â†’ 0.3)
                        # í•œêµ­ì–´ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
                        korean_name = self.FASHION_CLASS_MAPPING.get(class_name, class_name)
                        
                        # CLIP ê²€ì¦ (ê¸´íŒ”/ë°˜íŒ” êµ¬ë¶„ ë“±)
                        verified_class = self._verify_detection_with_clip(
                            pil_image, bbox, class_name, korean_name, clip_analyzer
                        ) if clip_analyzer else (class_name, korean_name)
                        
                        verified_class_en, verified_class_ko = verified_class
                        
                        detected_items.append({
                            "class": verified_class_ko,
                            "class_en": verified_class_en,
                            "confidence": confidence,
                            "bbox": bbox,
                            "original_class": class_name  # ì›ë³¸ í´ë˜ìŠ¤ë„ ì €ì¥ (ë””ë²„ê¹…ìš©)
                        })
                else:
                    # COCO ëª¨ë¸ì¸ ê²½ìš°: ê¸°ì¡´ í•„í„°ë§ ë¡œì§ ìœ ì§€
                    fashion_related = ['person', 'handbag', 'backpack', 'suitcase', 'sports ball']
                    if class_name in fashion_related or confidence > 0.3:
                        detected_items.append({
                            "class": class_name,
                            "confidence": confidence,
                            "bbox": bbox
                        })
        
        return {
            "items": detected_items,
            "image_size": image.size if isinstance(image, Image.Image) else (img_array.shape[1], img_array.shape[0]),
            "is_fashion_model": self.is_fashion_model
        }
    
    def _verify_detection_with_clip(self, image, bbox, class_name, korean_name, clip_analyzer):
        """CLIPì„ ì‚¬ìš©í•˜ì—¬ YOLO íƒì§€ ê²°ê³¼ ê²€ì¦ (íŠ¹íˆ ê¸´íŒ”/ë°˜íŒ” êµ¬ë¶„)"""
        try:
            # bboxë¡œ ì´ë¯¸ì§€ ì˜ì—­ ì˜ë¼ë‚´ê¸°
            x1, y1, x2, y2 = map(int, bbox)
            width, height = image.size
            
            # bbox ë²”ìœ„ í™•ì¸
            x1 = max(0, min(x1, width))
            y1 = max(0, min(y1, height))
            x2 = max(x1, min(x2, width))
            y2 = max(y1, min(y2, height))
            
            if x2 - x1 < 10 or y2 - y1 < 10:  # ë„ˆë¬´ ì‘ì€ ì˜ì—­ì€ ê±´ë„ˆë›°ê¸°
                return (class_name, korean_name)
            
            # ì˜ìƒ ì˜ì—­ ì¶”ì¶œ
            crop_image = image.crop((x1, y1, x2, y2))
            
            # ê¸´íŒ”/ë°˜íŒ” êµ¬ë¶„ ê²€ì¦
            if "sleeve" in class_name.lower() or "ìƒì˜" in korean_name:
                # ê¸´íŒ” vs ë°˜íŒ” ê²€ì¦
                long_sleeve_keywords = ["ê¸´íŒ”", "long sleeve", "ë¡± ìŠ¬ë¦¬ë¸Œ", "ì†Œë§¤ê°€ ê¸´"]
                short_sleeve_keywords = ["ë°˜íŒ”", "short sleeve", "ìˆ ìŠ¬ë¦¬ë¸Œ", "ì†Œë§¤ê°€ ì§§ì€", "íŒ”ì´ ë³´ì´ëŠ”"]
                
                # CLIPìœ¼ë¡œ ì‹¤ì œ íŒ” ê¸¸ì´ í™•ì¸
                test_keywords = ["long sleeve shirt", "short sleeve shirt", "ê¸´íŒ”", "ë°˜íŒ”"]
                similarity_result = clip_analyzer.analyze_style(crop_image, test_keywords)
                
                if similarity_result and similarity_result.get("text_matches"):
                    matches = similarity_result["text_matches"]
                    long_score = sum(v for k, v in matches.items() if any(word in k.lower() for word in ["long", "ê¸´"]))
                    short_score = sum(v for k, v in matches.items() if any(word in k.lower() for word in ["short", "ë°˜"]))
                    
                    # CLIP ê²€ì¦ ê²°ê³¼ê°€ YOLO ê²°ê³¼ì™€ ë‹¤ë¥´ë©´ ìˆ˜ì •
                    is_originally_long = "long" in class_name.lower() or "ê¸´íŒ”" in korean_name
                    
                    if short_score > long_score + 0.2 and is_originally_long:
                        # ë°˜íŒ”ë¡œ ìˆ˜ì •
                        if "top" in class_name:
                            return ("short sleeve top", "ë°˜íŒ” ìƒì˜")
                        elif "outwear" in class_name:
                            return ("short sleeve outwear", "ë°˜íŒ” ì•„ìš°í„°")
                        elif "dress" in class_name:
                            return ("short sleeve dress", "ë°˜íŒ” ë“œë ˆìŠ¤")
                    elif long_score > short_score + 0.2 and not is_originally_long:
                        # ê¸´íŒ”ë¡œ ìˆ˜ì •
                        if "top" in class_name:
                            return ("long sleeve top", "ê¸´íŒ” ìƒì˜")
                        elif "outwear" in class_name:
                            return ("long sleeve outwear", "ê¸´íŒ” ì•„ìš°í„°")
                        elif "dress" in class_name:
                            return ("long sleeve dress", "ê¸´íŒ” ë“œë ˆìŠ¤")
        except Exception as e:
            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í´ë˜ìŠ¤ ë°˜í™˜
            pass
        
        return (class_name, korean_name)

class CLIPAnalyzer:
    """CLIP ëª¨ë¸ì„ ì‚¬ìš©í•œ ìŠ¤íƒ€ì¼ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def detect_gender(self, image):
        """CLIPì„ ì‚¬ìš©í•œ ì„±ë³„ ì¸ì‹ (ê°œì„ : ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš©)"""
        try:
            # ë” êµ¬ì²´ì ì¸ ì„±ë³„ ê´€ë ¨ í‚¤ì›Œë“œ
            gender_texts = [
                "ë‚¨ì„± íŒ¨ì…˜", "ì—¬ì„± íŒ¨ì…˜", "ë‚¨ì ì˜·", "ì—¬ì ì˜·",
                "male clothing", "female clothing", "men's fashion", "women's fashion",
                "ë‚¨ì„± ì˜ìƒ", "ì—¬ì„± ì˜ìƒ", "ë‚¨ì„± ìŠ¤íƒ€ì¼", "ì—¬ì„± ìŠ¤íƒ€ì¼"
            ]
            similarities = self.analyze_style(image, gender_texts)
            
            if similarities and similarities.get("text_matches"):
                matches = similarities["text_matches"]
                # ë‚¨ì„± ê´€ë ¨ í‚¤ì›Œë“œ ì ìˆ˜ í•©ì‚°
                male_score = sum(v for k, v in matches.items() if any(word in k.lower() for word in ["ë‚¨ì„±", "ë‚¨ì", "male", "men"]))
                # ì—¬ì„± ê´€ë ¨ í‚¤ì›Œë“œ ì ìˆ˜ í•©ì‚°
                female_score = sum(v for k, v in matches.items() if any(word in k.lower() for word in ["ì—¬ì„±", "ì—¬ì", "female", "women"]))
                
                # ì„ê³„ê°’ ìƒí–¥: ë” í™•ì‹¤í•  ë•Œë§Œ íŒë³„
                score_diff = abs(male_score - female_score)
                if male_score > female_score and score_diff > 0.15:  # 0.15 ì´ìƒ ì°¨ì´
                    return "ë‚¨ì„±"
                elif female_score > male_score and score_diff > 0.15:
                    return "ì—¬ì„±"
                else:
                    return None  # ë¶ˆí™•ì‹¤í•˜ë©´ None ë°˜í™˜ (ì˜ìƒ ê¸°ë°˜ íŒë‹¨ì— ì˜ì¡´)
            return None
        except Exception as e:
            return None
    
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™”"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"CLIP ëª¨ë¸ ë¡œë“œ ì¤‘... (ì¥ì¹˜: {self.device})")
        
        try:
            # ëª¨ë¸ì„ ë¨¼ì € CPUì— ë¡œë“œí•œ í›„ deviceë¡œ ì´ë™
            # device_map="cpu"ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ í…ì„œ ë¬¸ì œ ë°©ì§€
            self.model = CLIPModel.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map=None  # ë¨¼ì € CPUì— ë¡œë“œ
            )
            self.processor = CLIPProcessor.from_pretrained(model_name)
            
            # ëª¨ë¸ì´ ì™„ì „íˆ ë¡œë“œëœ í›„ deviceë¡œ ì´ë™
            if self.device != "cpu":
                self.model = self.model.to(self.device)
            else:
                # CPUì¸ ê²½ìš° ì´ë¯¸ CPUì— ìˆìœ¼ë¯€ë¡œ ì´ë™ ë¶ˆí•„ìš”
                pass
                
            self.model.eval()
            print(f"CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name} (ì¥ì¹˜: {self.device})")
        except Exception as e:
            print(f"CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ì²« ì‹¤í–‰ ì‹œ ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            # ëŒ€ì²´ ë°©ë²• ì‹œë„
            try:
                print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
                self.model = CLIPModel.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32
                )
                self.processor = CLIPProcessor.from_pretrained(model_name)
                # device ì´ë™ ì—†ì´ CPUì—ì„œ ì‚¬ìš©
                self.device = "cpu"
                self.model.eval()
                print(f"CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (CPU ëª¨ë“œ): {model_name}")
            except Exception as e2:
                print(f"ëŒ€ì²´ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                raise
    
    def analyze_style(self, image, text_descriptions):
        """ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ê³¼ ìƒ‰ìƒ ë¶„ì„"""
        if not text_descriptions:
            text_descriptions = ["ìºì£¼ì–¼", "í¬ë©€", "íŠ¸ë Œë””", "ë¹¨ê°„ìƒ‰", "íŒŒë€ìƒ‰", "ê²€ì€ìƒ‰", "í°ìƒ‰"]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if isinstance(image, Image.Image):
            pass  # PIL ImageëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
        elif isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        else:
            raise ValueError(f"Unsupported image type: {type(image)}")
        
        try:
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            inputs = self.processor(
                text=text_descriptions,
                images=image,
                return_tensors="pt",
                padding=True
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
                image_features = outputs.image_embeds
                text_features = outputs.text_embeds
                
                # ì •ê·œí™”
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ìŠ¤ì¼€ì¼ë§)
                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
            
            # ê²°ê³¼ íŒŒì‹±
            similarities = similarity[0].cpu().numpy()
            text_matches = {
                desc: float(sim) for desc, sim in zip(text_descriptions, similarities)
            }
            
            # ê°€ì¥ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼ ì°¾ê¸°
            best_match_idx = similarities.argmax()
            best_style = text_descriptions[best_match_idx]
            best_score = float(similarities[best_match_idx])
            
            # ìƒ‰ìƒ ì¶”ì¶œ (ìƒ‰ìƒ ê´€ë ¨ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§)
            color_keywords = ["ë¹¨ê°„ìƒ‰", "íŒŒë€ìƒ‰", "ê²€ì€ìƒ‰", "í°ìƒ‰", "íšŒìƒ‰", "ë…¸ë€ìƒ‰", "ì´ˆë¡ìƒ‰", "ë¶„í™ìƒ‰"]
            color_matches = {k: text_matches.get(k, 0.0) for k in color_keywords if k in text_matches}
            dominant_color = max(color_matches.items(), key=lambda x: x[1])[0] if color_matches else "ì•Œ ìˆ˜ ì—†ìŒ"
            
            return {
                "style": best_style,
                "color": dominant_color,
                "pattern": "ì•Œ ìˆ˜ ì—†ìŒ",  # CLIPìœ¼ë¡œëŠ” íŒ¨í„´ ì¶”ì¶œì´ ì–´ë ¤ì›€
                "text_matches": text_matches,
                "confidence": best_score
            }
            
        except Exception as e:
            print(f"CLIP ë¶„ì„ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "style": "ì•Œ ìˆ˜ ì—†ìŒ",
                "color": "ì•Œ ìˆ˜ ì—†ìŒ",
                "pattern": "ì•Œ ìˆ˜ ì—†ìŒ",
                "text_matches": {},
                "confidence": 0.0,
                "error": str(e)
            }

class WeatherBasedRecommender:
    """ë‚ ì”¨ ê¸°ë°˜ ì½”ë”” ì¶”ì²œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        pass
    
    def get_weather_recommendation(self, temperature, weather, season):
        """ë‚ ì”¨ì™€ ê³„ì ˆì— ë§ëŠ” ì½”ë”” ì¶”ì²œ"""
        if temperature < 5:
            return {"type": "ê²¨ìš¸ ì½”ë””", "items": ["ì½”íŠ¸", "ìŠ¤ì›¨í„°", "ë¶€ì¸ "], "layer": "ë‹¤ì¸µ"}
        elif temperature < 15:
            return {"type": "ê°€ì„/ë´„ ì½”ë””", "items": ["ì¬í‚·", "ë‹ˆíŠ¸", "ìŠ¤ë‹ˆì»¤ì¦ˆ"], "layer": "ì¤‘ê°„"}
        else:
            return {"type": "ì—¬ë¦„ ì½”ë””", "items": ["í‹°ì…”ì¸ ", "ë°˜ë°”ì§€", "ìƒŒë“¤"], "layer": "ë‹¨ì¼"}

class MBTIAnalyzer:
    """MBTI ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.mbti_styles = {
            "ENFP": "ììœ ë¡­ê³  ì»¬ëŸ¬í’€í•œ ìºì£¼ì–¼ ìŠ¤íƒ€ì¼",
            "ISTJ": "ê¹”ë”í•˜ê³  ë‹¨ì •í•œ í¬ë©€ ìŠ¤íƒ€ì¼",
            "ESFP": "íŠ¸ë Œë””í•˜ê³  í™”ë ¤í•œ ìŠ¤íƒ€ì¼",
            "INTJ": "ë¯¸ë‹ˆë©€í•˜ê³  ì„¸ë ¨ëœ ìŠ¤íƒ€ì¼"
        }
    
    def get_personality_style(self, mbti_type):
        """MBTIì— ë§ëŠ” ìŠ¤íƒ€ì¼ ë°˜í™˜"""
        return self.mbti_styles.get(mbti_type, "ê· í˜•ì¡íŒ ìŠ¤íƒ€ì¼")

class TextBasedSearcher:
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì½”ë”” ê²€ìƒ‰ í´ë˜ìŠ¤ (CLIP í™œìš©)"""
    
    def __init__(self, clip_analyzer=None):
        """CLIP ë¶„ì„ê¸°ë¥¼ ì£¼ì…ë°›ê±°ë‚˜ ìƒˆë¡œ ìƒì„±"""
        self.clip_analyzer = clip_analyzer
        # ì„±ë³„ë³„ ì•„ì´í…œ ì¹´í…Œê³ ë¦¬
        self.outfit_categories = {
            "íŒŒí‹°ìš©": {
                "ë‚¨ì„±": ["í™”ë ¤í•œ ì •ì¥", "ì‹œí€¸ ì¬í‚·", "ìŠ¤íŒ½ê¸€ ì•¡ì„¸ì„œë¦¬", "ì •ì¥í™”"],
                "ì—¬ì„±": ["í™”ë ¤í•œ ë“œë ˆìŠ¤", "ì‹œí€¸ ì›í”¼ìŠ¤", "ìŠ¤íŒ½ê¸€ ì•¡ì„¸ì„œë¦¬"]
            },
            "ì¶œê·¼ë£©": {
                "ë‚¨ì„±": ["ì •ì¥ ì¬í‚·", "ì…”ì¸ ", "ìŠ¬ë™ìŠ¤", "ë¡œí¼"],
                "ì—¬ì„±": ["ì •ì¥ ì¬í‚·", "ë¸”ë¼ìš°ìŠ¤", "ìŠ¬ë™ìŠ¤", "ë¡œí¼"]
            },
            "ë°ì´íŠ¸ë£©": {
                "ë‚¨ì„±": ["ì„¸ë ¨ëœ ì…”ì¸ ", "ë¶€ë“œëŸ¬ìš´ ì»¬ëŸ¬ ì¬í‚·", "ìš°ì•„í•œ ì•¡ì„¸ì„œë¦¬"],
                "ì—¬ì„±": ["ë¡œë§¨í‹± ì›í”¼ìŠ¤", "ë¶€ë“œëŸ¬ìš´ ì»¬ëŸ¬", "ìš°ì•„í•œ ì•¡ì„¸ì„œë¦¬"]
            }
        }
    
    def search_outfits(self, query, reference_images=None, gender=None):
        """í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì½”ë”” ê²€ìƒ‰ (CLIP í™œìš©)"""
        # ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­
        matched_category = None
        for category in self.outfit_categories.keys():
            if category in query:
                matched_category = category
                break
        
        # ì„±ë³„ ê¸°ë³¸ê°’ ì„¤ì • (ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš°)
        if gender is None:
            gender = "ì—¬ì„±"  # ê¸°ë³¸ê°’ (í•˜ìœ„ í˜¸í™˜ì„±)
        
        # ì„±ë³„ì— ë§ëŠ” ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
        items = ["ìºì£¼ì–¼ ì›¨ì–´"]  # ê¸°ë³¸ê°’
        if matched_category and matched_category in self.outfit_categories:
            category_items = self.outfit_categories[matched_category]
            items = category_items.get(gender, category_items.get("ì—¬ì„±", ["ìºì£¼ì–¼ ì›¨ì–´"]))
        
        # CLIPì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­ (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
        if reference_images and self.clip_analyzer:
            # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            best_matches = []
            for img in reference_images:
                try:
                    result = self.clip_analyzer.analyze_style(img, [query])
                    if result.get("confidence", 0) > 0.1:
                        best_matches.append({
                            "image": img,
                            "similarity": result.get("confidence", 0),
                            "style": result.get("style", "")
                        })
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
                    continue
            
            if best_matches:
                # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                best_matches.sort(key=lambda x: x["similarity"], reverse=True)
                return {
                    "category": matched_category or "ì¼ë°˜",
                    "items": items,
                    "matched": True,
                    "clip_results": best_matches[:3]  # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
                }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ ë°˜í™˜
        return {
            "category": matched_category or "ì¼ë°˜",
            "items": items,
            "matched": matched_category is not None
        }

class FashionRecommender:
    """í†µí•© íŒ¨ì…˜ ì½”ë”” ì¶”ì²œ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ëª¨ë“  ì¶”ì²œ ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        print("íŒ¨ì…˜ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self.detector = YOLODetector()
        self.analyzer = CLIPAnalyzer()
        self.weather_recommender = WeatherBasedRecommender()
        self.mbti_analyzer = MBTIAnalyzer()
        self.text_searcher = TextBasedSearcher(clip_analyzer=self.analyzer)
        print("íŒ¨ì…˜ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def recommend_outfit(self, image, mbti, temperature, weather, season):
        """í†µí•© ì½”ë”” ì¶”ì²œ íŒŒì´í”„ë¼ì¸"""
        # 1. YOLOv5ë¡œ ì˜· ì•„ì´í…œ íƒì§€ (CLIP ê²€ì¦ í¬í•¨)
        detected_items = self.detector.detect_clothes(image, clip_analyzer=self.analyzer)
        
        # 2. CLIPìœ¼ë¡œ ìŠ¤íƒ€ì¼ ë° ìƒ‰ìƒ ë¶„ì„
        style_descriptions = ["ìºì£¼ì–¼", "í¬ë©€", "íŠ¸ë Œë””", "ìŠ¤í¬ì¸ ", "ë¹ˆí‹°ì§€", "ëª¨ë˜"]
        # ìƒ‰ìƒ í‚¤ì›Œë“œ í™•ì¥ (í•œêµ­ì–´ + ì˜ì–´)
        color_descriptions = [
            "ë¹¨ê°„ìƒ‰", "íŒŒë€ìƒ‰", "ê²€ì€ìƒ‰", "í°ìƒ‰", "íšŒìƒ‰", "ê°ˆìƒ‰", "ë² ì´ì§€",
            "ë…¸ë€ìƒ‰", "ì˜ë¡œìš°", "yellow",  # ì¶”ê°€
            "ë³´ë¼ìƒ‰", "í¼í”Œ", "purple",
            "ì˜¤ë Œì§€", "ì£¼í™©ìƒ‰", "orange",
            "ì´ˆë¡ìƒ‰", "ê·¸ë¦°", "green",
            "ë¶„í™ìƒ‰", "í•‘í¬", "pink",
            "ë„¤ì´ë¹„", "navy",
            "ì¹´í‚¤", "khaki",
            "white", "black", "red", "blue"  # ì˜ì–´ ê¸°ë³¸ ìƒ‰ìƒ
        ]
        all_descriptions = style_descriptions + color_descriptions
        style_analysis = self.analyzer.analyze_style(image, all_descriptions)
        
        # 3. ë‚ ì”¨/ê³„ì ˆ ì •ë³´ ê³ ë ¤
        weather_rec = self.weather_recommender.get_weather_recommendation(temperature, weather, season)
        
        # 4. MBTI ê°œì¸í™” ì ìš©
        mbti_style = self.mbti_analyzer.get_personality_style(mbti)
        
        # 5. íƒì§€ëœ ì•„ì´í…œ ê¸°ë°˜ ì¶”ì²œ ìƒì„±
        outfit_combinations = []
        
        # ìŠ¤íƒ€ì¼ë³„ ì¶”ì²œ ìƒì„±
        for style in style_descriptions:
            if style in style_analysis.get("text_matches", {}):
                confidence = style_analysis["text_matches"][style]
                if confidence > 0.1:  # ìœ ì˜ë¯¸í•œ ìœ ì‚¬ë„ë§Œ
                    outfit_combinations.append({
                        "style": style,
                        "items": weather_rec["items"],
                        "confidence": confidence,
                        "detected_items": detected_items["items"][:3] if detected_items["items"] else []
                    })
        
        # ì¶”ì²œì´ ì ìœ¼ë©´ ê¸°ë³¸ ì¶”ì²œ ì¶”ê°€
        if len(outfit_combinations) < 3:
            for style in ["ìºì£¼ì–¼", "í¬ë©€", "íŠ¸ë Œë””"]:
                if not any(oc["style"] == style for oc in outfit_combinations):
                    outfit_combinations.append({
                        "style": style,
                        "items": weather_rec["items"],
                        "confidence": 0.5,
                        "detected_items": []
                    })
        
        # confidence ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        outfit_combinations.sort(key=lambda x: x["confidence"], reverse=True)
        outfit_combinations = outfit_combinations[:3]  # ìƒìœ„ 3ê°œë§Œ
        
        return {
            "detected_items": detected_items,
            "style_analysis": style_analysis,
            "weather_recommendation": weather_rec,
            "mbti_style": mbti_style,
            "outfit_combinations": outfit_combinations
        }
