"""
ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ - ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì— ì¶”ì²œ ì½”ë”” í•©ì„±
YOLO íƒì§€ â†’ ì•„ì´í…œë³„ ìƒì„± â†’ ì˜ì—­ í•©ì„± â†’ ìƒ‰ìƒ ë³´ì •
"""

import cv2
import numpy as np
from PIL import Image
import torch
from typing import Dict, List, Tuple, Optional
from diffusers import StableDiffusionInpaintPipeline
from diffusers import DPMSolverMultistepScheduler
from .common_utils import get_device_info, extract_color_from_text, extract_color_bgr, COLOR_MAP


class VirtualFittingSystem:
    """ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ - ì‚¬ìš©ì ì´ë¯¸ì§€ì— ì¶”ì²œ ì½”ë”” í•©ì„±"""
    
    def __init__(self, yolo_detector, clip_analyzer):
        """
        Args:
            yolo_detector: YOLODetector ì¸ìŠ¤í„´ìŠ¤
            clip_analyzer: CLIPAnalyzer ì¸ìŠ¤í„´ìŠ¤
        """
        self.yolo_detector = yolo_detector
        self.clip_analyzer = clip_analyzer
        self.inpaint_pipe = None  # inpainting íŒŒì´í”„ë¼ì¸ (í•„ìš” ì‹œ ë¡œë“œ)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        self.device, self.vae_device = get_device_info()
        if self.device == "mps":
            print("ğŸ MPS (GPU) ì‚¬ìš© ê°€ëŠ¥ - ë¹ ë¥¸ ì´ë¯¸ì§€ ìƒì„±")
        else:
            print("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    def detect_clothing_regions(self, image: Image.Image) -> Dict:
        """
        YOLOë¡œ ì˜ë¥˜ ì˜ì—­ íƒì§€
        
        Returns:
            {
                "top": {"bbox": [x1, y1, x2, y2], "class": "...", "confidence": 0.9},
                "bottom": {"bbox": [...], ...},
                "person": {"bbox": [...], ...}
            }
        """
        # YOLO íƒì§€ ì‹¤í–‰
        result = self.yolo_detector.detect_clothes(image)
        items = result.get("items", [])
        
        regions = {}
        
        # íƒì§€ëœ ì•„ì´í…œì„ ìƒì˜/í•˜ì˜/ì „ì‹ ìœ¼ë¡œ ë¶„ë¥˜
        for item in items:
            class_name = item.get("class", "").lower()
            class_en = item.get("class_en", "").lower()
            bbox = item.get("bbox", [])
            
            if not bbox or len(bbox) != 4:
                continue
            
            # ìƒì˜ ë¶„ë¥˜
            if any(keyword in class_name or keyword in class_en 
                   for keyword in ["ìƒì˜", "top", "shirt", "t-shirt", "jacket", "outwear"]):
                if "top" not in regions or item.get("confidence", 0) > regions["top"].get("confidence", 0):
                    regions["top"] = {
                        "bbox": bbox,
                        "class": item.get("class", ""),
                        "confidence": item.get("confidence", 0)
                    }
            
            # í•˜ì˜ ë¶„ë¥˜
            elif any(keyword in class_name or keyword in class_en 
                     for keyword in ["í•˜ì˜", "bottom", "pants", "ë°”ì§€", "skirt", "ì¹˜ë§ˆ"]):
                if "bottom" not in regions or item.get("confidence", 0) > regions["bottom"].get("confidence", 0):
                    regions["bottom"] = {
                        "bbox": bbox,
                        "class": item.get("class", ""),
                        "confidence": item.get("confidence", 0)
                    }
            
            # ì „ì‹  (person)
            elif "person" in class_name or "person" in class_en:
                if "person" not in regions or item.get("confidence", 0) > regions["person"].get("confidence", 0):
                    regions["person"] = {
                        "bbox": bbox,
                        "class": item.get("class", ""),
                        "confidence": item.get("confidence", 0)
                    }
        
        return regions
    
    def expand_bbox(self, bbox: List[int], image_size: Tuple[int, int], padding: float = 0.1) -> List[int]:
        """
        ë°”ìš´ë”©ë°•ìŠ¤ í™•ì¥ (ì—¬ìœ  ê³µê°„ ì¶”ê°€)
        
        Args:
            bbox: [x1, y1, x2, y2]
            image_size: (width, height)
            padding: í™•ì¥ ë¹„ìœ¨ (0.1 = 10%)
        
        Returns:
            í™•ì¥ëœ [x1, y1, x2, y2]
        """
        x1, y1, x2, y2 = bbox
        width, height = image_size
        
        w = x2 - x1
        h = y2 - y1
        
        # íŒ¨ë”© ì ìš©
        pad_w = int(w * padding)
        pad_h = int(h * padding)
        
        x1 = max(0, x1 - pad_w)
        y1 = max(0, y1 - pad_h)
        x2 = min(width, x2 + pad_w)
        y2 = min(height, y2 + pad_h)
        
        return [x1, y1, x2, y2]
    
    def create_mask_from_bbox(self, image_size: Tuple[int, int], bbox: List[int]) -> np.ndarray:
        """
        ë°”ìš´ë”©ë°•ìŠ¤ë¡œë¶€í„° ë§ˆìŠ¤í¬ ìƒì„±
        
        Returns:
            mask: (height, width) 0 ë˜ëŠ” 255
        """
        width, height = image_size
        mask = np.zeros((height, width), dtype=np.uint8)
        
        x1, y1, x2, y2 = bbox
        mask[y1:y2, x1:x2] = 255
        
        # ë¶€ë“œëŸ¬ìš´ ì—£ì§€ (ë¸”ë Œë”© ê°œì„ )
        mask = cv2.GaussianBlur(mask, (15, 15), 0)
        
        return mask
    
    def apply_color_correction(self, source: np.ndarray, target: np.ndarray, 
                              mask: np.ndarray) -> np.ndarray:
        """
        ìƒ‰ìƒ ë³´ì • - í•©ì„±ëœ ì˜ì—­ì˜ ìƒ‰ìƒ/ì¡°ëª…ì„ ì›ë³¸ê³¼ ì¼ì¹˜
        
        Args:
            source: í•©ì„±í•  ì´ë¯¸ì§€ (BGR)
            target: ì›ë³¸ ì´ë¯¸ì§€ (BGR)
            mask: í•©ì„± ì˜ì—­ ë§ˆìŠ¤í¬
        
        Returns:
            ë³´ì •ëœ ì´ë¯¸ì§€ (BGR)
        """
        result = target.copy()
        
        # ë§ˆìŠ¤í¬ ì˜ì—­ì—ì„œ íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­
        mask_3ch = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
        
        # ê° ì±„ë„ë³„ íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­
        for i in range(3):
            # íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­ (OpenCV ë‚´ì¥ í•¨ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ê°„ë‹¨íˆ ë°ê¸° ì¡°ì •)
            source_mean = np.mean(source[:, :, i][mask > 0])
            target_mean = np.mean(target[:, :, i][mask > 0]) if np.sum(mask > 0) > 0 else source_mean
            
            if source_mean > 0:
                scale = target_mean / source_mean
                source[:, :, i] = np.clip(source[:, :, i] * scale, 0, 255).astype(np.uint8)
        
        # Alpha blending
        mask_normalized = mask.astype(float) / 255.0
        for i in range(3):
            result[:, :, i] = (source[:, :, i] * mask_normalized + 
                              target[:, :, i] * (1 - mask_normalized)).astype(np.uint8)
        
        return result
    
    def _load_inpaint_pipeline(self):
        """Stable Diffusion Inpainting íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if self.inpaint_pipe is not None:
            return
        
        print("ğŸ¨ Stable Diffusion Inpainting ëª¨ë¸ ë¡œë“œ ì¤‘...")
        print("   - ì˜ë¥˜ ì˜ì—­ì„ ì‹¤ì œë¡œ êµì²´í•©ë‹ˆë‹¤ (ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ê°€ ì•„ë‹˜)")
        print("   - ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì•½ 5GB, ëª‡ ë¶„ ì†Œìš”)")
        print(f"   - ì¥ì¹˜: {self.device.upper()} ëª¨ë“œ ({'ë¹ ë¦„' if self.device == 'mps' else 'ëŠë¦¼'})")
        
        try:
            self.inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(
                "stabilityai/stable-diffusion-2-inpainting",
                torch_dtype=torch.float32,
                safety_checker=None,
                device_map=None
            )
            
            # PNDM ëŒ€ì‹  ë” ë¹ ë¥´ê³  ì•ˆì •ì ì¸ DPM Solver ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
            self.inpaint_pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                self.inpaint_pipe.scheduler.config
            )
            
            # ë””ë°”ì´ìŠ¤ ë°°ì¹˜ (MPS: UNetë§Œ, CPU: VAE/TextEncoder)
            if self.device == "mps":
                self.inpaint_pipe.unet = self.inpaint_pipe.unet.float().to(self.device, non_blocking=False)
                self.inpaint_pipe.vae = self.inpaint_pipe.vae.to(self.vae_device, non_blocking=False)
                self.inpaint_pipe.text_encoder = self.inpaint_pipe.text_encoder.float().to("cpu", non_blocking=False)
                
                # MPS íŒ¨ì¹˜ ì ìš© (ìˆœì„œ ì¤‘ìš”: VAE íŒ¨ì¹˜ ë¨¼ì €)
                self._patch_vae_for_mps()
                self._apply_mps_patches()
                
                print("âœ… Inpainting ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (MPS/GPU ëª¨ë“œ, DPM Solver ìŠ¤ì¼€ì¤„ëŸ¬)")
            else:
                # CPU ëª¨ë“œ
                self.inpaint_pipe.unet = self.inpaint_pipe.unet.to("cpu")
                self.inpaint_pipe.vae = self.inpaint_pipe.vae.to("cpu")
                self.inpaint_pipe.text_encoder = self.inpaint_pipe.text_encoder.to("cpu")
                print("âœ… Inpainting ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (CPU ëª¨ë“œ, DPM Solver ìŠ¤ì¼€ì¤„ëŸ¬)")
        except Exception as e:
            print(f"âš ï¸ Inpainting ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            self.inpaint_pipe = None
    
    def _patch_vae_for_mps(self):
        """VAEì˜ encode/decode ë©”ì„œë“œë¥¼ íŒ¨ì¹˜í•˜ì—¬ MPSì™€ í˜¸í™˜ë˜ë„ë¡"""
        if self.device != "mps":
            return
        
        # VAE encode íŒ¨ì¹˜
        original_encode = self.inpaint_pipe.vae.encode
        
        def patched_vae_encode(self_vae, x, return_dict=True, **kwargs):
            # VAEëŠ” CPUì— ìˆìœ¼ë¯€ë¡œ ì…ë ¥ì€ CPUë¡œ
            if x.device.type != "cpu":
                x = x.to("cpu", non_blocking=False)
            # VAE encode ì‹¤í–‰
            result = original_encode(x, return_dict=return_dict, **kwargs)
            # latentsë¥¼ MPSë¡œ ì´ë™ (í•„ìš”í•œ ê²½ìš°)
            if return_dict:
                if hasattr(result, 'latent_dist'):
                    # latent_distì˜ sampleì„ MPSë¡œ ì´ë™
                    pass  # sample() í˜¸ì¶œ ì‹œ ì²˜ë¦¬
                return result
            else:
                # íŠœí”Œ ë°˜í™˜ì¸ ê²½ìš°
                if isinstance(result, tuple):
                    return tuple(r.to(self.device, non_blocking=False) if isinstance(r, torch.Tensor) and r.device.type != self.device else r for r in result)
                return result.to(self.device, non_blocking=False) if isinstance(result, torch.Tensor) and result.device.type != self.device else result
        
        self.inpaint_pipe.vae.encode = patched_vae_encode.__get__(self.inpaint_pipe.vae, type(self.inpaint_pipe.vae))
        
        # VAE decode íŒ¨ì¹˜ (ê°•í™”ëœ ë²„ì „) - generator ì¸ì ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
        original_decode = self.inpaint_pipe.vae.decode
        
        # ì›ë³¸ í•¨ìˆ˜ì˜ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ì„ ìœ„í•´ inspect ì‚¬ìš©
        import inspect
        sig = inspect.signature(original_decode)
        print(f"   ğŸ“‹ VAE decode ì›ë³¸ ì‹œê·¸ë‹ˆì²˜: {sig}")
        
        def patched_vae_decode(self_vae, z, return_dict=True, generator=None, **kwargs):
            # generator ì¸ìë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë°›ë˜ ë¬´ì‹œ (ì—ëŸ¬ ë°©ì§€)
            # ì…ë ¥ì„ CPUë¡œ ì´ë™
            if z.device.type != "cpu":
                z = z.to("cpu", non_blocking=False)
            # generatorì™€ ê´€ë ¨ëœ ëª¨ë“  ì¸ì ì œê±°
            kwargs.pop('generator', None)
            # ì›ë³¸ decode í˜¸ì¶œ (generator ì œì™¸)
            return original_decode(z, return_dict=return_dict, **kwargs)
        
        self.inpaint_pipe.vae.decode = patched_vae_decode.__get__(self.inpaint_pipe.vae, type(self.inpaint_pipe.vae))
        
        print("   âœ… VAE encode/decode íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
    
    def _apply_mps_patches(self):
        """MPS ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ íŒ¨ì¹˜ ì ìš©"""
        if self.device != "mps":
            return
        
        # UNet forward íŒ¨ì¹˜
        original_unet_forward = self.inpaint_pipe.unet.forward
        
        def patched_unet_forward(self_unet, sample, timestep, encoder_hidden_states=None, **kwargs):
            # ëª¨ë“  ì…ë ¥ì„ MPSë¡œ ì´ë™
            if sample.device.type != self.device:
                sample = sample.to(self.device, non_blocking=False)
            if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                timestep = timestep.to(self.device, non_blocking=False)
            if encoder_hidden_states is not None and encoder_hidden_states.device.type != self.device:
                encoder_hidden_states = encoder_hidden_states.to(self.device, non_blocking=False)
            
            # kwargsì˜ í…ì„œë„ MPSë¡œ
            for key, value in kwargs.items():
                if isinstance(value, torch.Tensor) and value.device.type != self.device:
                    kwargs[key] = value.to(self.device, non_blocking=False)
            
            return original_unet_forward(sample, timestep, encoder_hidden_states, **kwargs)
        
        self.inpaint_pipe.unet.forward = patched_unet_forward.__get__(self.inpaint_pipe.unet, type(self.inpaint_pipe.unet))
        
        # Scheduler step íŒ¨ì¹˜
        original_scheduler_step = self.inpaint_pipe.scheduler.step
        
        def patched_scheduler_step(self_scheduler, model_output, timestep, sample, **kwargs):
            if model_output.device.type != self.device:
                model_output = model_output.to(self.device, non_blocking=False)
            if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                timestep = timestep.to(self.device, non_blocking=False)
            if sample.device.type != self.device:
                sample = sample.to(self.device, non_blocking=False)
            
            return original_scheduler_step(model_output, timestep, sample, **kwargs)
        
        self.inpaint_pipe.scheduler.step = patched_scheduler_step.__get__(self.inpaint_pipe.scheduler, type(self.inpaint_pipe.scheduler))
        
        # Inpainting íŒŒì´í”„ë¼ì¸ì˜ __call__ ë©”ì„œë“œ íŒ¨ì¹˜ (ê°€ì¥ ì¤‘ìš”!)
        original_call = self.inpaint_pipe.__call__
        
        def patched_call(self_pipe, prompt=None, image=None, mask_image=None, **kwargs):
            # ëª¨ë“  ì…ë ¥ ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ë¥¼ CPUì—ì„œ ì²˜ë¦¬ (VAEê°€ CPUì— ìˆìœ¼ë¯€ë¡œ)
            # í•˜ì§€ë§Œ latent ìƒì„± í›„ì—ëŠ” MPSë¡œ ì´ë™í•´ì•¼ í•¨
            
            # ì›ë³¸ í˜¸ì¶œ
            result = original_call(prompt=prompt, image=image, mask_image=mask_image, **kwargs)
            return result
        
        # Inpainting íŒŒì´í”„ë¼ì¸ì˜ _encode_vae_image íŒ¨ì¹˜ (masked_image_latentsë¥¼ MPSë¡œ ì´ë™)
        if hasattr(self.inpaint_pipe, '_encode_vae_image'):
            original_encode_vae_image = self.inpaint_pipe._encode_vae_image
            
            def patched_encode_vae_image(self_pipe, image, generator):
                # VAEëŠ” CPUì— ìˆìœ¼ë¯€ë¡œ ì´ë¯¸ì§€ë„ CPUë¡œ
                if image.device.type != "cpu":
                    image = image.to("cpu", non_blocking=False)
                # VAE encode ì‹¤í–‰
                result = original_encode_vae_image(image, generator)
                # ê²°ê³¼ë¥¼ MPSë¡œ ì´ë™
                if isinstance(result, torch.Tensor):
                    if result.device.type != self.device:
                        result = result.to(self.device, non_blocking=False)
                return result
            
            self.inpaint_pipe._encode_vae_image = patched_encode_vae_image.__get__(self.inpaint_pipe, type(self.inpaint_pipe))
        
        # ê°€ì¥ ì¤‘ìš”í•œ íŒ¨ì¹˜: prepare_mask_latentsì™€ prepare_latents
        # Inpainting íŒŒì´í”„ë¼ì¸ì˜ ë‚´ë¶€ ë©”ì„œë“œë¥¼ ì§ì ‘ íŒ¨ì¹˜
        import types
        
        # prepare_mask_latents íŒ¨ì¹˜ (ì˜¬ë°”ë¥¸ ì‹œê·¸ë‹ˆì²˜)
        if hasattr(self.inpaint_pipe, 'prepare_mask_latents'):
            original_prepare_mask_latents = self.inpaint_pipe.prepare_mask_latents
            
            def patched_prepare_mask_latents(self_pipe, mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance):
                # deviceë¥¼ MPSë¡œ ê°•ì œ
                device = torch.device(self.device)
                # ì›ë³¸ í˜¸ì¶œ
                mask_latents, masked_image_latents = original_prepare_mask_latents(
                    mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance
                )
                # ê²°ê³¼ë¥¼ MPSë¡œ ì´ë™
                if mask_latents.device.type != self.device:
                    mask_latents = mask_latents.to(self.device, non_blocking=False)
                if masked_image_latents.device.type != self.device:
                    masked_image_latents = masked_image_latents.to(self.device, non_blocking=False)
                return mask_latents, masked_image_latents
            
            self.inpaint_pipe.prepare_mask_latents = types.MethodType(patched_prepare_mask_latents, self.inpaint_pipe)
        
        # prepare_latents íŒ¨ì¹˜ (ì‚¬ì „ì— ì ìš©, ì˜¬ë°”ë¥¸ ì‹œê·¸ë‹ˆì²˜)
        if hasattr(self.inpaint_pipe, 'prepare_latents'):
            original_prepare_latents = self.inpaint_pipe.prepare_latents
            
            def patched_prepare_latents(self_pipe, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None, image=None, timestep=None, is_strength_max=True, return_noise=False, return_image_latents=False):
                # deviceë¥¼ MPSë¡œ ê°•ì œ
                device = torch.device(self.device)
                result = original_prepare_latents(
                    batch_size, num_channels_latents, height, width, dtype, device, generator, 
                    latents, image, timestep, is_strength_max, return_noise, return_image_latents
                )
                # ê²°ê³¼ë¥¼ MPSë¡œ ì´ë™
                if isinstance(result, tuple):
                    result = tuple(r.to(self.device, non_blocking=False) if isinstance(r, torch.Tensor) and r.device.type != self.device else r for r in result)
                elif isinstance(result, torch.Tensor) and result.device.type != self.device:
                    result = result.to(self.device, non_blocking=False)
                return result
            
            self.inpaint_pipe.prepare_latents = types.MethodType(patched_prepare_latents, self.inpaint_pipe)
        
        # __call__ ë©”ì„œë“œ íŒ¨ì¹˜ ë¶ˆí•„ìš” - VAE decode íŒ¨ì¹˜ë¡œ ì¶©ë¶„
        # (ì´ë¯¸ _patch_vae_for_mpsì—ì„œ ì²˜ë¦¬ë¨)
        
        print("   âœ… MPS íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
    
    def composite_outfit_on_image(self, original_image: Image.Image, 
                                 outfit_items: List[str],
                                 gender: str = "ë‚¨ì„±") -> Optional[Image.Image]:
        """
        ì›ë³¸ ì´ë¯¸ì§€ì— ì¶”ì²œ ì½”ë””ë¥¼ í•©ì„±
        
        Args:
            original_image: ì‚¬ìš©ì ì—…ë¡œë“œ ì´ë¯¸ì§€
            outfit_items: ì¶”ì²œ ì½”ë”” ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë¹¨ê°„ìƒ‰ ê¸´íŒ” ì…”ì¸ ", "ê²€ì€ìƒ‰ ë°”ì§€"])
            gender: ì„±ë³„
        
        Returns:
            í•©ì„±ëœ ì´ë¯¸ì§€ ë˜ëŠ” None
        """
        try:
            print("ğŸ¨ ê°€ìƒ í”¼íŒ… ì‹œì‘...")
            print(f"   - ì•„ì´í…œ: {outfit_items}")
            print(f"   - ì„±ë³„: {gender}")
            
            # 1. ì˜ë¥˜ ì˜ì—­ íƒì§€
            regions = self.detect_clothing_regions(original_image)
            
            print(f"   - íƒì§€ëœ ì˜ì—­: {list(regions.keys())}")
            
            if not regions:
                print("âš ï¸ ì˜ë¥˜ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ì˜ì—­ì´ ì—†ì–´ë„ ì›ë³¸ ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                result_image = self._create_text_overlay_image(original_image, outfit_items)
                return result_image, []  # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì—†ìŒ
            
            # 2. OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            img_cv = cv2.cvtColor(np.array(original_image), cv2.COLOR_RGB2BGR)
            height, width = img_cv.shape[:2]
            
            # 3. Inpaintingìœ¼ë¡œ ì‹¤ì œ ì˜ë¥˜ í•©ì„±
            # Inpainting íŒŒì´í”„ë¼ì¸ ë¡œë“œ
            self._load_inpaint_pipeline()
            
            if self.inpaint_pipe is None:
                print("âš ï¸ Inpainting ëª¨ë¸ ì—†ìŒ. ê°„ë‹¨í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ ì‚¬ìš©")
                result_image = self._simple_color_overlay(img_cv, regions, outfit_items, width, height)
                return result_image, []  # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì—†ìŒ
            
            # Inpaintingìœ¼ë¡œ ê° ì•„ì´í…œ í•©ì„± (ìƒì˜ + í•˜ì˜ ëª¨ë‘ ì²˜ë¦¬)
            result_pil = original_image.copy()
            prompts_info = []  # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì €ì¥
            
            # ìƒì˜ì™€ í•˜ì˜ ëª¨ë‘ ì²˜ë¦¬ (ìµœëŒ€ 2ê°œ)
            for idx, item in enumerate(outfit_items[:2]):  # ìƒì˜ + í•˜ì˜
                region_type = "top" if idx == 0 else "bottom"
                
                if region_type not in regions:
                    print(f"âš ï¸ {region_type} ì˜ì—­ ì—†ìŒ, ë‹¤ìŒ ì•„ì´í…œìœ¼ë¡œ")
                    continue
                
                bbox = regions[region_type]["bbox"]
                x1, y1, x2, y2 = [int(v) for v in bbox]
                
                # ë§ˆìŠ¤í¬ ìƒì„± (Inpaintingìš©)
                mask_pil = Image.new("L", (width, height), 0)  # ê²€ì€ìƒ‰
                from PIL import ImageDraw
                draw = ImageDraw.Draw(mask_pil)
                draw.rectangle([x1, y1, x2, y2], fill=255)  # í°ìƒ‰ = êµì²´í•  ì˜ì—­
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„± (region_type ì „ë‹¬!)
                prompt = self._build_inpaint_prompt(item, gender, region_type)
                
                # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì €ì¥
                region_name = "ìƒì˜" if region_type == "top" else "í•˜ì˜"
                prompts_info.append({
                    "region": region_name,
                    "prompt": prompt
                })
                
                # ì„±ë³„ì— ë”°ë¥¸ negative prompt ê°•í™”
                if gender == "ë‚¨ì„±":
                    negative_prompt = (
                        "woman, female, women's clothing, women's shoes, high heels, "
                        "breasts, cleavage, feminine curves, "
                        "wrong color, mismatched clothes, double clothing, overlay, blur, "
                        "distorted body, unrealistic fabric, old outfit, wrong gender clothing, "
                        "face, head, portrait, drawing, painting, illustration, cartoon, "
                        "anime, unrealistic, fake, artificial, CGI, 3D render, computer graphics"
                    )
                else:  # ì—¬ì„±
                    negative_prompt = (
                        "man, male, men's clothing, men's shoes, "
                        "wrong color, mismatched clothes, double clothing, overlay, blur, "
                        "distorted body, unrealistic fabric, old outfit, wrong gender clothing, "
                        "face, head, portrait, drawing, painting, illustration, cartoon, "
                        "anime, unrealistic, fake, artificial, CGI, 3D render, computer graphics"
                    )
                
                print(f"ğŸ¨ {region_type} ì˜ì—­ Inpainting ì¤‘...")
                print(f"   - í”„ë¡¬í”„íŠ¸: {prompt}")
                
                try:
                    # ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ ìµœì  í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ì†ë„ í–¥ìƒ)
                    # ì›ë³¸ í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ ë¦¬ì‚¬ì´ì¦ˆ (ë„ˆë¬´ í¬ë©´ ëŠë¦¼)
                    max_size = 512  # ìµœëŒ€ í¬ê¸° ì œí•œ
                    orig_w, orig_h = original_image.size
                    
                    # ë¦¬ì‚¬ì´ì¦ˆ í•„ìš” ì—¬ë¶€ í™•ì¸
                    needs_resize = max(orig_w, orig_h) > max_size
                    
                    if needs_resize:
                        ratio = max_size / max(orig_w, orig_h)
                        target_size = (int(orig_w * ratio), int(orig_h * ratio))
                        # í•œ ë²ˆë§Œ ë¦¬ì‚¬ì´ì¦ˆ
                        result_pil_for_inpaint = result_pil.resize(target_size, Image.Resampling.LANCZOS)
                        mask_pil_for_inpaint = mask_pil.resize(target_size, Image.Resampling.LANCZOS)
                        print(f"   - ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {original_image.size} â†’ {target_size}")
                    else:
                        # ë¦¬ì‚¬ì´ì¦ˆ ë¶ˆí•„ìš”
                        result_pil_for_inpaint = result_pil
                        mask_pil_for_inpaint = mask_pil
                        print(f"   - ì›ë³¸ í¬ê¸° ì‚¬ìš©: {original_image.size}")
                    
                    # Inpainting ì‹¤í–‰ (DPM SolverëŠ” ë” ì ì€ ìŠ¤í…ìœ¼ë¡œë„ ì¢‹ì€ ê²°ê³¼)
                    # ìŠ¤í… ìˆ˜ ì¡°ì •: IndexError ë°©ì§€ë¥¼ ìœ„í•´ 11ë¡œ ì„¤ì • (DPM SolverëŠ” ë‚´ë¶€ì ìœ¼ë¡œ +1ì„ ì‚¬ìš©)
                    num_steps = 11 if self.device == "mps" else 7
                    
                    with torch.no_grad():
                        try:
                            # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” (ë§¤ë²ˆ ìƒˆë¡œ ì‹œì‘)
                            if hasattr(self.inpaint_pipe.scheduler, 'set_timesteps'):
                                self.inpaint_pipe.scheduler.set_timesteps(num_steps, device=self.device)
                            
                            result = self.inpaint_pipe(
                                prompt=prompt,
                                negative_prompt=negative_prompt,
                                image=result_pil_for_inpaint,
                                mask_image=mask_pil_for_inpaint,
                                num_inference_steps=num_steps,  # ê°ì†Œëœ ìŠ¤í… ìˆ˜
                                guidance_scale=7.5,  # ì ì ˆí•œ ê°€ì´ë˜ìŠ¤ (9.0 â†’ 7.5)
                                strength=0.85  # ì•½ê°„ ë‚®ì¶¤ (0.9 â†’ 0.85)
                            )
                        except (RuntimeError, TypeError) as e:
                            error_str = str(e)
                            if "unexpected keyword argument" in error_str and "generator" in error_str:
                                # VAE decode ì‹œê·¸ë‹ˆì²˜ ì˜¤ë¥˜ - íŒ¨ì¹˜ ì¬ì ìš© ë° ì¬ì‹œë„
                                print(f"   âš ï¸ VAE decode ì‹œê·¸ë‹ˆì²˜ ì˜¤ë¥˜, íŒ¨ì¹˜ ì¬ì ìš© ì¤‘...")
                                # VAE decode íŒ¨ì¹˜ ì¬ì ìš©
                                original_decode = self.inpaint_pipe.vae.decode
                                def patched_vae_decode_fix(self_vae, z, return_dict=True, **kwargs):
                                    if z.device.type != "cpu":
                                        z = z.to("cpu", non_blocking=False)
                                    # generator ì¸ì ì œê±°
                                    kwargs.pop('generator', None)
                                    return original_decode(z, return_dict=return_dict, **kwargs)
                                self.inpaint_pipe.vae.decode = patched_vae_decode_fix.__get__(self.inpaint_pipe.vae, type(self.inpaint_pipe.vae))
                                # ì¬ì‹œë„ (ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”)
                                num_steps = 12 if self.device == "mps" else 8
                                if hasattr(self.inpaint_pipe.scheduler, 'set_timesteps'):
                                    self.inpaint_pipe.scheduler.set_timesteps(num_steps, device=self.device)
                                result = self.inpaint_pipe(
                                    prompt=prompt,
                                    negative_prompt=negative_prompt,
                                    image=result_pil_for_inpaint,
                                    mask_image=mask_pil_for_inpaint,
                                    num_inference_steps=num_steps,
                                    guidance_scale=7.5,
                                    strength=0.85
                                )
                            elif "must be on the same device" in error_str or "same device" in error_str:
                                # ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜ - MPS íŒ¨ì¹˜ ì¬ì ìš©
                                print(f"   âš ï¸ ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜, MPS íŒ¨ì¹˜ ì¬ì ìš© ì¤‘...")
                                # íŒ¨ì¹˜ ì¬ì ìš©
                                self._apply_mps_patches()
                                # ì¬ì‹œë„ (ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”)
                                num_steps = 12 if self.device == "mps" else 8
                                if hasattr(self.inpaint_pipe.scheduler, 'set_timesteps'):
                                    self.inpaint_pipe.scheduler.set_timesteps(num_steps, device=self.device)
                                result = self.inpaint_pipe(
                                    prompt=prompt,
                                    negative_prompt=negative_prompt,
                                    image=result_pil_for_inpaint,
                                    mask_image=mask_pil_for_inpaint,
                                    num_inference_steps=num_steps,
                                    guidance_scale=7.5,
                                    strength=0.85
                                )
                            elif "list index out of range" in error_str or "IndexError" in error_str:
                                # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì˜¤ë¥˜ - ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ì´ˆê¸°í™”
                                print(f"   âš ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì˜¤ë¥˜, ì¬ì´ˆê¸°í™” ì¤‘...")
                                # ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ìƒì„±
                                from diffusers import DPMSolverMultistepScheduler
                                self.inpaint_pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                                    self.inpaint_pipe.scheduler.config
                                )
                                # ì¬ì‹œë„ (IndexError ë°©ì§€ë¥¼ ìœ„í•´ ìŠ¤í… ìˆ˜ ì¡°ì •)
                                num_steps = 11 if self.device == "mps" else 7
                                if hasattr(self.inpaint_pipe.scheduler, 'set_timesteps'):
                                    self.inpaint_pipe.scheduler.set_timesteps(num_steps, device=self.device)
                                result = self.inpaint_pipe(
                                    prompt=prompt,
                                    negative_prompt=negative_prompt,
                                    image=result_pil_for_inpaint,
                                    mask_image=mask_pil_for_inpaint,
                                    num_inference_steps=num_steps,
                                    guidance_scale=7.5,
                                    strength=0.85
                                )
                            else:
                                # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¬ë°œìƒ
                                print(f"   âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {error_str[:100]}")
                                raise
                    
                    # ê²°ê³¼ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                    generated = result.images[0]
                    
                    # ë¦¬ì‚¬ì´ì¦ˆëœ ê²½ìš°ì—ë§Œ ì›ë³¸ í¬ê¸°ë¡œ ë³µì› (í•œ ë²ˆë§Œ)
                    if needs_resize and generated.size != original_image.size:
                        generated = generated.resize(original_image.size, Image.Resampling.LANCZOS)
                        mask_pil_full = mask_pil.resize(original_image.size, Image.Resampling.LANCZOS)
                    else:
                        # ë¦¬ì‚¬ì´ì¦ˆí•˜ì§€ ì•Šì€ ê²½ìš° ë§ˆìŠ¤í¬ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        mask_pil_full = mask_pil
                    
                    # ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ í•©ì„± (ë‚˜ë¨¸ì§€ëŠ” ì›ë³¸ ìœ ì§€)
                    result_np = np.array(result_pil)
                    generated_np = np.array(generated)
                    
                    mask_np = np.array(mask_pil_full) > 127  # ì´ì§„ ë§ˆìŠ¤í¬ (boolean)
                    mask_3d = np.stack([mask_np] * 3, axis=2).astype(float)  # 0.0 ë˜ëŠ” 1.0
                    
                    # ë§ˆìŠ¤í¬ ì˜ì—­ì€ ìƒì„±ëœ ì´ë¯¸ì§€, ë‚˜ë¨¸ì§€ëŠ” ì›ë³¸
                    # mask_3dê°€ 1ì¸ ì˜ì—­ = ìƒì„±ëœ ì´ë¯¸ì§€, 0ì¸ ì˜ì—­ = ì›ë³¸
                    blended = result_np.astype(float) * (1.0 - mask_3d) + generated_np.astype(float) * mask_3d
                    result_np = np.clip(blended, 0, 255).astype(np.uint8)
                    
                    result_pil = Image.fromarray(result_np)
                    
                    print(f"âœ… {region_type} ì˜ì—­ Inpainting ì™„ë£Œ (ì‹¤ì œ í•©ì„±ë¨)")
                    print(f"   - ë§ˆìŠ¤í¬ ì˜ì—­ í¬ê¸°: {np.sum(mask_np)} í”½ì…€")
                    
                except Exception as e:
                    print(f"âš ï¸ Inpainting ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
                    # í´ë°±: ê°„ë‹¨í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´
                    result_image = self._simple_color_overlay(img_cv, regions, outfit_items, width, height)
                    return result_image, []  # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì—†ìŒ
            
            print("âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ (Inpainting)")
            # í”„ë¡¬í”„íŠ¸ ì •ë³´ì™€ í•¨ê»˜ ë°˜í™˜
            return result_pil, prompts_info
            
        except Exception as e:
            print(f"âš ï¸ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None, []  # í”„ë¡¬í”„íŠ¸ ì •ë³´ ì—†ìŒ
    
    def _build_inpaint_prompt(self, item_text: str, gender: str, region_type: str = "top") -> str:
        """
        Inpaintingìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (êµ¬ì²´ì ì´ê³  ì‹œê°ì ì¸ ì§€ì‹œë¬¸)
        
        Args:
            item_text: ì•„ì´í…œ ì„¤ëª… (ì˜ˆ: "ë¹¨ê°„ìƒ‰ ê¸´íŒ” ì…”ì¸ ")
            gender: ì„±ë³„ ("ë‚¨ì„±" ë˜ëŠ” "ì—¬ì„±")
            region_type: "top" ë˜ëŠ” "bottom"
        
        Returns:
            Inpainting í”„ë¡¬í”„íŠ¸
        """
        # ìƒ‰ìƒ ë³€í™˜ (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        
        # ì˜ë¥˜ íƒ€ì… ë° ì¬ì§ˆ ë³€í™˜
        item_map = {
            "ë°˜íŒ”": "short sleeve", "ê¸´íŒ”": "long sleeve",
            "í‹°ì…”ì¸ ": "t-shirt", "í‹°": "t-shirt", "ì…”ì¸ ": "shirt",
            "ë°”ì§€": "pants", "íŒ¬ì¸ ": "pants", "ë°˜ë°”ì§€": "shorts",
            "ì¬í‚·": "jacket", "ìì¼“": "jacket", "ê°€ë””ê±´": "cardigan",
            "ì½”íŠ¸": "coat", "íŠ¸ë Œì¹˜ì½”íŠ¸": "trench coat",
            "ì²­ë°”ì§€": "jeans", "ì§„": "jeans",
            "ìŠ¤ë‹ˆì»¤ì¦ˆ": "sneakers", "ìŠ¤ë‹ˆì»¤": "sneakers",
            "ë¶€ì¸ ": "boots", "ì‹ ë°œ": "shoes",
            "ì„ ê¸€ë¼ìŠ¤": "sunglasses", "ì•ˆê²½": "glasses",
            "ë¦°ë„¨": "linen", "ë©´": "cotton", "ìš¸": "wool",
            "ë‹ˆíŠ¸": "knit", "ìŠ¤ì›¨í„°": "sweater"
        }
        
        # ì¬ì§ˆ ì¶”ì¶œ
        fabric_map = {
            "ë©´": "cotton", "ë¦°ë„¨": "linen", "ìš¸": "wool", "ë‹ˆíŠ¸": "knit",
            "ë°ë‹˜": "denim", "ì²­": "denim", "ê°€ì£½": "leather", "ì‹¤í¬": "silk"
        }
        
        # ë³€í™˜
        en_item = item_text
        item_text_lower = item_text.lower()
        
        # ìƒ‰ìƒ ì¶”ì¶œ (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        extracted_color = extract_color_from_text(item_text)
        if extracted_color:
            # ìƒ‰ìƒëª… ì œê±°í•˜ì—¬ íƒ€ì…ë§Œ ë‚¨ê¹€
            for kr, en in COLOR_MAP.items():
            if kr in item_text:
                    en_item = en_item.replace(kr, "").strip()
                if en.lower() in item_text_lower:
                    en_item = en_item.replace(en, "").strip()
        
        # ì˜ë¥˜ íƒ€ì… ì¶”ì¶œ (ë” ì •í™•í•˜ê²Œ)
        extracted_type = None
        # ê¸´íŒ”/ë°˜íŒ” ë¨¼ì € í™•ì¸
        if "ê¸´íŒ”" in item_text or "long sleeve" in item_text_lower:
            extracted_type = "long sleeve"
        elif "ë°˜íŒ”" in item_text or "short sleeve" in item_text_lower:
            extracted_type = "short sleeve"
        
        # ê·¸ ë‹¤ìŒ ì…”ì¸ /í‹°ì…”ì¸ /ë°”ì§€ ë“± í™•ì¸
        for kr, en in item_map.items():
            if kr in item_text:
                if extracted_type:
                    # ì´ë¯¸ ê¸´íŒ”/ë°˜íŒ”ì´ ìˆìœ¼ë©´ ì¡°í•©
                    if "sleeve" in en:
                        extracted_type = f"{extracted_type} {en.replace('sleeve', '').strip()}"
                    else:
                        extracted_type = f"{extracted_type} {en}"
                else:
                extracted_type = en
                en_item = en_item.replace(kr, "")
        
        # ì¬ì§ˆ ì¶”ì¶œ
        extracted_fabric = None
        for kr, en in fabric_map.items():
            if kr in item_text:
                extracted_fabric = en
                break
        
        # ë‚¨ì€ í•œê¸€ ë‹¨ì–´ ì œê±°
        import re
        en_item = re.sub(r'[ê°€-í£]+', '', en_item).strip()
        en_item = re.sub(r'\s+', ' ', en_item).strip()
        en_item = re.sub(r'\s*(ë˜ëŠ”|or)\s*.*', '', en_item, flags=re.IGNORECASE).strip()
        
        # ì„±ë³„ ëª…í™•íˆ ì§€ì •
        gender_kw = "man" if gender == "ë‚¨ì„±" else "woman" if gender == "ì—¬ì„±" else "person"
        
        # êµ¬ì²´ì ì´ê³  ì‹œê°ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒ‰ìƒê³¼ íƒ€ì… ì •í™•íˆ ëª…ì‹œ)
        if region_type == "top":
            # ìƒì˜
            if extracted_type and extracted_color:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                # íƒ€ì… ì •í™•íˆ ì§€ì •
                if "long sleeve" in extracted_type.lower() or "ê¸´íŒ”" in item_text:
                    type_spec = "long sleeve shirt"
                elif "short sleeve" in extracted_type.lower() or "ë°˜íŒ”" in item_text:
                    type_spec = "short sleeve t-shirt"
                elif "t-shirt" in extracted_type.lower() or "í‹°" in item_text:
                    type_spec = "t-shirt"
                else:
                    type_spec = "shirt"
                
                # ìƒ‰ìƒì´ ì •í™•íˆ ë°˜ì˜ë˜ë„ë¡ ê°•ì¡°
                prompt = (
                    f"a {gender_kw} wearing a {extracted_color} {type_spec}, "
                    f"EXACTLY {extracted_color} color, {fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
                print(f"   ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„±: ìƒ‰ìƒ={extracted_color}, íƒ€ì…={type_spec}")
            elif extracted_type:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                if "long sleeve" in extracted_type or "ê¸´íŒ”" in item_text:
                    type_spec = "long sleeve shirt"
                elif "short sleeve" in extracted_type or "ë°˜íŒ”" in item_text:
                    type_spec = "short sleeve t-shirt"
                else:
                    type_spec = "shirt"
                
                prompt = (
                    f"a {gender_kw} wearing {type_spec}, "
                    f"{fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
            else:
                prompt = (
                    f"a {gender_kw} wearing upper body clothing, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
        else:
            # í•˜ì˜
            if extracted_type and extracted_color:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                # íƒ€ì… ì •í™•íˆ ì§€ì •
                if "pants" in extracted_type.lower() or "ë°”ì§€" in item_text:
                    type_spec = "slim-fit trousers"
                elif "shorts" in extracted_type.lower() or "ë°˜ë°”ì§€" in item_text:
                    type_spec = "shorts"
                else:
                    type_spec = "pants"
                
                # ìƒ‰ìƒì´ ì •í™•íˆ ë°˜ì˜ë˜ë„ë¡ ê°•ì¡°
                prompt = (
                    f"a {gender_kw} wearing {extracted_color} {type_spec}, "
                    f"EXACTLY {extracted_color} color, {fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
            elif extracted_type:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                if "pants" in extracted_type or "ë°”ì§€" in item_text:
                    type_spec = "slim-fit trousers"
                elif "shorts" in extracted_type or "ë°˜ë°”ì§€" in item_text:
                    type_spec = "shorts"
                else:
                    type_spec = "pants"
                
                prompt = (
                    f"a {gender_kw} wearing {type_spec}, "
                    f"{fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
            else:
                prompt = (
                    f"a {gender_kw} wearing lower body clothing, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
        
        return prompt
    
    def _simple_color_overlay(self, img_cv: np.ndarray, regions: Dict, 
                             outfit_items: List[str], width: int, height: int) -> Image.Image:
        """
        í´ë°±: ê°„ë‹¨í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ (Inpainting ì‹¤íŒ¨ ì‹œ)
        """
        result_img = img_cv.copy()
        
        for idx, item in enumerate(outfit_items[:2]):
            region_type = "top" if idx == 0 else "bottom"
            
            if region_type not in regions:
                continue
            
            bbox = regions[region_type]["bbox"]
            x1, y1, x2, y2 = [int(v) for v in bbox]
            
            color_bgr = self._extract_target_color(item)
            
            if color_bgr is not None:
                roi = result_img[y1:y2, x1:x2].copy()
                roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
                colored_roi = np.full_like(roi, color_bgr, dtype=np.uint8)
                
                for c in range(3):
                    colored_roi[:, :, c] = np.clip(
                        colored_roi[:, :, c] * (roi_gray.astype(float) / 128.0),
                        0, 255
                    ).astype(np.uint8)
                
                alpha = 0.8
                blended_roi = cv2.addWeighted(colored_roi, alpha, roi, 1-alpha, 0)
                result_img[y1:y2, x1:x2] = blended_roi
                
                print(f"âœ… {region_type} ì˜ì—­ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ ì ìš©")
        
        return Image.fromarray(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))
    
    def _extract_target_color(self, item_text: str) -> Optional[Tuple[int, int, int]]:
        """
        ì•„ì´í…œ í…ìŠ¤íŠ¸ì—ì„œ ëª©í‘œ ìƒ‰ìƒ ì¶”ì¶œ (BGR)
        
        Returns:
            (B, G, R) ë˜ëŠ” None
        """
        return extract_color_bgr(item_text)
    
    def _create_text_overlay_image(self, image: Image.Image, items: List[str]) -> Image.Image:
        """
        ì˜ë¥˜ íƒì§€ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€
            items: ì¶”ì²œ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€
        """
        from PIL import ImageDraw
        
        # PIL ì´ë¯¸ì§€ ë³µì‚¬
        img_with_text = image.copy()
        draw = ImageDraw.Draw(img_with_text)
        
        # í…ìŠ¤íŠ¸ ì¶”ê°€
        text_lines = ["ì¶”ì²œ ì½”ë””:"] + items
        y_offset = 20
        
        for line in text_lines:
            # ë°°ê²½ ë°•ìŠ¤
            text_bbox = draw.textbbox((10, y_offset), line)
            draw.rectangle(
                [(text_bbox[0]-5, text_bbox[1]-5), (text_bbox[2]+5, text_bbox[3]+5)], 
                fill=(255, 255, 255)
            )
            # í…ìŠ¤íŠ¸
            draw.text((10, y_offset), line, fill=(0, 0, 0))
            y_offset += 25
        
        return img_with_text
